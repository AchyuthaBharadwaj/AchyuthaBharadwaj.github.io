---
layout: post
title: "Sparse Auto Encoder"
date: 2018-03-16
tags: [Machine Learning, Sparse Auto Encoder, Python]
excerpt: "Created a sparse autoencoder and implemented semi-supervised learning with MNIST data." 
mathjax: "true"
image: Intro-to-ML-4/sae-banner.png
---

In this project I created a sparse autoencoder and implement semi-supervised learning with MNIST data.

##  Sparse Autoencoder

[link to repository](https://github.com/AchyuthaBharadwaj/Machine-Learning/tree/master/Sparse%20Auto%20Encoder)

Sparse auto encoder:

![]({{ site.url }}{{ site.baseurl }}/img/Intro-to-ML-4/sae-1.png)

Architecture of the sae:

![]({{ site.url }}{{ site.baseurl }}/img/Intro-to-ML-4/sae-details.JPG)

Dataset: MNIST - All the digits <br/>
Autoencoder parameters:<br/>
Num of hidden layers: 1<br/>
hidden layer Size = 200<br/>
max iterations = 400<br/>
Sparsity parameter $$p = [0.01, 0.1, 0.5, 0.8]$$<br/>
Weight decay $$λ = 0.001$$<br/>
Sparsity coefficient $$β = 3$$<br/>

After training the auto encoder, the mosaic of the weights I obtained is as follows;

![]({{ site.url }}{{ site.baseurl }}/img/Intro-to-ML-4/sae-weights.png)

